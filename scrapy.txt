Scrapy终端(Scrapy shell)
  该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。
  可以测试任何的Python代码
   IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。

Item Pipeline
  当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理
  每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。
  以下是item pipeline的一些典型应用
    清理HTML数据
    验证爬取的数据(检查item包含某些字段)
    查重(并丢弃)
    将爬取结果保存到数据库中
  每个item pipiline组件是一个独立的Python类，同时必须实现以下方法:
    process_item(item, spider)
    每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem 异常，被丢弃的item将不会被之后的pipeline组件所处理。

    参数:
    item (Item 对象) – 被爬取的item
    spider (Spider 对象) – 爬取该item的spider

    open_spider(spider)
      当spider被开启时，这个方法被调用。

      参数:	spider (Spider 对象) – 被开启的spider
      close_spider(spider)
        当spider被关闭时，这个方法被调用

        参数:	spider (Spider 对象) – 被关闭的spider

        JsonWriterPipeline的目的只是为了介绍怎样编写item pipeline，如果你想要将所有爬取的item都保存到同一个JSON文件， 你需要使用 Feed exports 。

      启用一个Item Pipeline组件
      ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,
}分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。


Feed exports
  生成一个带有爬取数据的”输出文件”(通常叫做”输出feed”)，来供其他系统使用
  使用feed输出时您可以通过使用 URI (通过 FEED_URI 设置) 来定义存储端。 feed输出支持URI方式支持的多种存储后端类型。

  存储URI也包含参数。当feed被创建时这些参数可以被覆盖:

%(time)s - 当feed被创建时被timestamp覆盖
%(name)s - 被spider的名字覆盖

Requests and Responses

Settings

      Scrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。

      命令行选项(Command line Options)(最高优先级)
      每个spider的设定
      项目设定模块(Project settings module)
      命令默认设定模块(Default settings per-command)
      全局默认设定(Default global settings) (最低优先级

异常(Exceptions)
DropItem
  该异常由item pipeline抛出，用于停止处理item。

Logging

我们坚信，如果有些事情已经做得很好了，那就没必要再重复制造轮子
与其自行解决每个问题，我们选择从其他已经很好地解决问题的项目中复制想法(copy idea) ，并把注意力放在真正需要解决的问题上。

模拟用户登录
 使用FormRequest.from_response()方法模拟用户登录.

 在Scrapy中，类似Requests, Response及Items的对象具有有限的生命周期: 他们被创建，使用，最后被销毁。

这些对象中，Request的生命周期应该是最长的，其会在调度队列(Scheduler queue)中一直等待，直到被处理。 更多内容请参考 架构概览 。
为了帮助调试内存泄露，Scrapy提供了跟踪对象引用的机制，叫做 trackref

Scrapy为下载item中包含的文件(比如在爬取到产品时，同时也想保存对应的图片)提供了一个可重用的 item pipelines . 这些pipeline有些共同的方法和结构(我们称之为media pipeline)。一般来说你会使用Files Pipeline或者 Images Pipeline.

下载延迟0

Jobs: 暂停，恢复爬虫
    暂停爬取，之后再恢复运行
    一个把调度请求保存在磁盘的调度器
    一个把访问请求保存在磁盘的副本过滤器[duplicates filter]
    一个能持续保持爬虫状态(键/值对)的扩展

架构概览
    Scrapy Engine
        引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。
    调度器(Scheduler)
        调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎
    下载器(Downloader)
        下载器负责获取页面数据并提供给引擎，而后提供给spider
    Spiders
        Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站
    Item Pipeline
        Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中
    下载器中间件(Downloader middlewares)
        下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩
        展Scrapy功能

    Spider中间件(Spider middlewares)
            Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。

            数据流(Data flow)

            Scrapy中的数据流由执行引擎控制，其过程如下:

            引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。
            引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。
            引擎向调度器请求下一个要爬取的URL。
            调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。
            一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。
            引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。
            Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。
            引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。
            (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。

        下载器中间件(Downloader Middleware
        编写您自己的下载器中间件
        class scrapy.downloadermiddlewares.DownloaderMiddleware
process_request(request, spider)
当每个request通过下载中间件时，该方法被调用。

process_request() 必须返回其中之一: 返回 None 、返回一个 Response 对象、返回一个 Request 对象或raise IgnoreRequest 。
如果其返回 None ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。

如果其返回 Response 对象，Scrapy将不会调用 任何 其他的 process_request() 或 process_exception() 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。

信号(Signals)
    Scrapy使用信号来通知事情发生

    当spider进入空闲(idle)状态时该信号被发送。空闲意味着:
    requests正在等待被下载
requests被调度
items正在item pipeline中被处理
将scrapy变成一个可以在多个主机上同时运行的分布式爬虫。


NotImplementedError	尚未实现的方法

流是一种抽象概念，它代表了数据的无结构化传递。按照流的方式进行输入输出，数据被当成无结构的字节序或字符序列。从流中取得数据的操作称为提取操作，而向流中添加数据的操作称为插入操作。用来进行输入输出操作的流就称为IO流。换句话说，IO流就是以流的方式进行输入输出。


scrapy  微博逻辑

    启动 scrapy runspider  master.py (包含spider类：基类爬虫，所有spider都必须继承； 读mysql 和redis-队列相关； scrapy 爬虫框架)
    Spider就是您定义爬取的动作及分析某个网页


start_requests()
    返回一个可迭代对象(iterable)包含了spider用于爬取的第一个Request。

    当spider启动爬取并且未制定URL时，该方法被调用指定了URL时，make_requests_from_url() 将被调用来创建Request对象。 该方法仅仅会被Scrapy调用一次，因此您可以将其实现为生成器。
    该方法的默认实现是使用 start_urls 的url生成Request。
    parse(response)
    当response没有指定回调函数时，该方法是Scrapy处理下载的response的默认方法。

    为了自己能够研究得更加深入和对爬虫有更全面的了解，自己动手去多做

